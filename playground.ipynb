{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9d47e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install datasets --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e25ea863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-04 10:12:39.085013: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/conda/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import gaussian_diffusion as gd\n",
    "from step_sample import create_named_schedule_sampler\n",
    "from train_util import TrainLoop\n",
    "\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast, BertTokenizerFast, set_seed\n",
    "import json, torch, os\n",
    "from utils import dist_util\n",
    "from functools import partial\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "419eea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab_list.pickle', 'rb') as handle:\n",
    "    vocab_list = pickle.load(handle)\n",
    "vocab_list = list(vocab_list.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7cd8af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_util.clear_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fb13702",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.0001\n",
    "batch_size=64\n",
    "microbatch=20\n",
    "epochs=100\n",
    "eval_interval=1000\n",
    "ema_rate='0.9999' \n",
    "schedule_sampler='uniform'\n",
    "diffusion_steps=1000\n",
    "noise_schedule='sqrt'\n",
    "vocab='custom'\n",
    "use_plm_init='no' # embedding in transformer\n",
    "vocab_size=0\n",
    "config_name='bert-base-uncased'\n",
    "data_dir='data'\n",
    "seq_len=128\n",
    "hidden_t_dim=128\n",
    "hidden_dim=64\n",
    "dropout=0.1\n",
    "seed=102\n",
    "weight_decay=0.0\n",
    "predict_xstart=True\n",
    "rescale_timesteps=True\n",
    "emb_scale_factor=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffa896cd-2b1c-4ffe-8dbc-6fe4bc03ecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "423b336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myTokenizer():\n",
    "    \"\"\"\n",
    "    Load tokenizer from bert config or defined BPE vocab dict\n",
    "    \"\"\"\n",
    "    ################################################\n",
    "    ### You can custome your own tokenizer here. ###\n",
    "    ################################################\n",
    "    def __init__(self, vocab, config_name, custom_vocab_fp=None, custom_vocab_list=None):\n",
    "        if vocab == 'bert':\n",
    "            print(custom_vocab_fp or custom_vocab_list)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(config_name, vocab_file=custom_vocab_fp)\n",
    "            self.tokenizer = tokenizer\n",
    "            self.sep_token_id = tokenizer.sep_token_id\n",
    "            self.pad_token_id = tokenizer.pad_token_id\n",
    "            self.tokenizer.add_tokens(custom_vocab_list)\n",
    "        elif vocab == 'custom':\n",
    "            tokenizer = BertTokenizerFast('shakespeare-tokenizer-bert/vocab.txt')\n",
    "            self.tokenizer = tokenizer\n",
    "            self.sep_token_id = tokenizer.sep_token_id\n",
    "            self.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        self.vocab_size = len(self.tokenizer)\n",
    "    \n",
    "    def encode_token(self, sentences):\n",
    "        if isinstance(self.tokenizer, dict):\n",
    "            input_ids = [[0] + [self.tokenizer.get(x, self.tokenizer['[UNK]']) for x in seq.split()] + [1] for seq in sentences]\n",
    "        elif isinstance(self.tokenizer, PreTrainedTokenizerFast):\n",
    "            input_ids = self.tokenizer(sentences, add_special_tokens=True)['input_ids']\n",
    "        else:\n",
    "            assert False, \"invalid type of vocab_dict\"\n",
    "        return input_ids\n",
    "        \n",
    "    def decode_token(self, seq):\n",
    "        if isinstance(self.tokenizer, dict):\n",
    "            seq = seq.squeeze(-1).tolist()\n",
    "            while len(seq)>0 and seq[-1] == self.pad_token_id:\n",
    "                seq.pop()\n",
    "            tokens = \" \".join([self.rev_tokenizer[x] for x in seq]).replace('__ ', '').replace('@@ ', '')\n",
    "        elif isinstance(self.tokenizer, PreTrainedTokenizerFast):\n",
    "            seq = seq.squeeze(-1).tolist()\n",
    "            while len(seq)>0 and seq[-1] == self.pad_token_id:\n",
    "                seq.pop()\n",
    "            tokens = self.tokenizer.decode(seq)\n",
    "        else:\n",
    "            assert False, \"invalid type of vocab_dict\"\n",
    "        return tokens\n",
    "\n",
    "\n",
    "def load_model_emb(hidden_dim, tokenizer):\n",
    "    ### random emb or pre-defined embedding like glove embedding. You can custome your own init here.\n",
    "    model = torch.nn.Embedding(tokenizer.vocab_size, hidden_dim)\n",
    "    torch.nn.init.normal_(model.weight)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def load_tokenizer(vocab, config_name, custom_vocab_fp=None, custom_vocab_list=None):\n",
    "    tokenizer = myTokenizer(vocab, config_name, custom_vocab_fp=custom_vocab_fp, custom_vocab_list=custom_vocab_list)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1af61464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import psutil\n",
    "import datasets\n",
    "from datasets import Dataset as Dataset2\n",
    "\n",
    "def load_data_text(\n",
    "    batch_size, \n",
    "    seq_len, \n",
    "    data_dir,\n",
    "    deterministic=False, \n",
    "    data_args=None, \n",
    "    model_emb=None,\n",
    "    split='train', \n",
    "    loaded_vocab=None,\n",
    "    loop=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    For a dataset, create a generator over (seqs, kwargs) pairs.\n",
    "\n",
    "    Each seq is an (bsz, len, h) float tensor, and the kwargs dict contains zero or\n",
    "    more keys, each of which map to a batched Tensor of their own.\n",
    "    The kwargs dict can be used for some meta information.\n",
    "\n",
    "    :param batch_size: the batch size of each returned pair.\n",
    "    :param seq_len: the max sequence length (one-side).\n",
    "    :param deterministic: if True, yield results in a deterministic order.\n",
    "    :param data_args: including dataset directory, num of dataset, basic settings, etc.\n",
    "    :param model_emb: loaded word embeddings.\n",
    "    :param loaded_vocab: loaded word vocabs.\n",
    "    :param loop: loop to get batch data or not.\n",
    "    \"\"\"\n",
    "\n",
    "    print('#'*30, '\\nLoading text data...')\n",
    "\n",
    "    training_data = get_corpus(data_dir, seq_len, split=split, loaded_vocab=loaded_vocab)\n",
    "\n",
    "    dataset = TextDataset(\n",
    "        training_data,\n",
    "        model_emb=model_emb\n",
    "    )\n",
    "\n",
    "    if split != 'test':\n",
    "        data_loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,  # 20,\n",
    "            # drop_last=True,\n",
    "#             sampler=sampler,\n",
    "            # shuffle=not deterministic,\n",
    "            num_workers=4,\n",
    "        )\n",
    "    else:\n",
    "        data_loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,  # 20,\n",
    "            # drop_last=True,\n",
    "            # sampler=sampler,\n",
    "            shuffle=not deterministic,\n",
    "            num_workers=4,\n",
    "        )\n",
    "\n",
    "    if loop:\n",
    "        return infinite_loader(data_loader)\n",
    "    else:\n",
    "        # print(data_loader)\n",
    "        return iter(data_loader)\n",
    "\n",
    "def infinite_loader(data_loader):\n",
    "    while True:\n",
    "        yield from data_loader\n",
    "\n",
    "def helper_tokenize(sentence_lst, vocab_dict, seq_len):\n",
    "    # Process.memory_info is expressed in bytes, so convert to megabytes\n",
    "    print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "    raw_datasets = Dataset2.from_dict(sentence_lst)\n",
    "    print('This is raw_datasets: ', raw_datasets)\n",
    "    print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        input_id_x = vocab_dict.encode_token(examples['src'])\n",
    "        input_id_y = vocab_dict.encode_token(examples['trg'])\n",
    "        result_dict = {'input_id_x': input_id_x, 'input_id_y': input_id_y}\n",
    "\n",
    "        return result_dict\n",
    "\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        num_proc=4,\n",
    "        remove_columns=['src', 'trg'],\n",
    "        load_from_cache_file=True,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "    print('### tokenized_datasets', tokenized_datasets)\n",
    "    print('### tokenized_datasets...example', tokenized_datasets['input_id_x'][0])\n",
    "    print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "    def merge_and_mask(group_lst):\n",
    "        lst = []\n",
    "        mask = []\n",
    "        for i in range(len(group_lst['input_id_x'])):\n",
    "            end_token = group_lst['input_id_x'][i][-1]\n",
    "            src = group_lst['input_id_x'][i][:-1]\n",
    "            trg = group_lst['input_id_y'][i][:-1]\n",
    "            \n",
    "            # if source sentence and target sentence have a combined sequence length of more than the seq_len (set by us)\n",
    "            while len(src) + len(trg) > seq_len - 3:\n",
    "                if len(src)>len(trg):\n",
    "                    src.pop()\n",
    "                elif len(src)<len(trg):\n",
    "                    trg.pop()\n",
    "                else:\n",
    "                    src.pop()\n",
    "                    trg.pop()\n",
    "            src.append(end_token)\n",
    "            trg.append(end_token)\n",
    "\n",
    "            # append the input ids of source sentence + seperator id + input ids of target sentence\n",
    "            # example: [2, 150, 12941, 142, 137, 205, 150, 7309, 130, 1015, 3, 3, 2, 605, 137, 26, 408, 119, 2971, 645, 93, 7012, 3]\n",
    "            lst.append(src + [vocab_dict.sep_token_id] + trg)\n",
    "            # mask : a list of 0, length of list = number of source tokens + 1\n",
    "            mask.append([0]*(len(src)+1))\n",
    "        group_lst['input_ids'] = lst\n",
    "        group_lst['input_mask'] = mask\n",
    "        return group_lst\n",
    "    \n",
    "    tokenized_datasets = tokenized_datasets.map(\n",
    "        merge_and_mask,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        desc=f\"merge and mask\",\n",
    "    )\n",
    "    \n",
    "    def pad_function(group_lst):\n",
    "        max_length = seq_len\n",
    "        group_lst['input_ids'] = _collate_batch_helper(group_lst['input_ids'], vocab_dict.pad_token_id, max_length)\n",
    "        group_lst['input_mask'] = _collate_batch_helper(group_lst['input_mask'], 1, max_length)\n",
    "        return group_lst\n",
    "\n",
    "    print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "    lm_datasets = tokenized_datasets.map(\n",
    "        pad_function,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        desc=f\"padding\",\n",
    "    )\n",
    "\n",
    "    print(lm_datasets, 'padded dataset')\n",
    "    print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "    raw_datasets = datasets.DatasetDict()\n",
    "    raw_datasets['train'] = lm_datasets\n",
    "    print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "    return raw_datasets\n",
    "\n",
    "\n",
    "def get_corpus(data_dir, seq_len, split='train', loaded_vocab=None):\n",
    "\n",
    "    print('#'*30, '\\nLoading dataset from {}...'.format(data_dir))\n",
    "\n",
    "    sentence_lst = {'src':[], 'trg': []}\n",
    "    \n",
    "    if split == 'train':\n",
    "        print('### Loading form the TRAIN set...')\n",
    "        path = f'{data_dir}/train.jsonl'\n",
    "    elif split == 'train-one':\n",
    "        print('### Loading form the ONE TRAIN set...')\n",
    "        path = f'{data_dir}/train-one.jsonl'\n",
    "    elif split == 'valid':\n",
    "        print('### Loading form the VALID set...')\n",
    "        path = f'{data_dir}/valid.jsonl'\n",
    "    elif split == 'test':\n",
    "        print('### Loading form the TEST set...')\n",
    "        path = f'{data_dir}/test.jsonl'\n",
    "    else:\n",
    "        assert False, \"invalid split for dataset\"\n",
    "\n",
    "    with open(path, 'r') as f_reader:\n",
    "        for row in f_reader:\n",
    "            content = json.loads(row)\n",
    "            sentence_lst['src'].append(content['src'].strip())\n",
    "            sentence_lst['trg'].append(content['trg'].strip())\n",
    "\n",
    "    print('### Data samples...\\n', sentence_lst['src'][:2], sentence_lst['trg'][:2])\n",
    "        \n",
    "    # get tokenizer.\n",
    "    vocab_dict = loaded_vocab\n",
    "\n",
    "    train_dataset = helper_tokenize(sentence_lst, vocab_dict, seq_len)\n",
    "    return train_dataset\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_datasets, model_emb=None):\n",
    "        super().__init__()\n",
    "        self.text_datasets = text_datasets\n",
    "        self.length = len(self.text_datasets['train'])\n",
    "        self.model_emb = model_emb\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            input_ids = self.text_datasets['train'][idx]['input_ids']\n",
    "            hidden_state = self.model_emb(torch.tensor(input_ids))\n",
    "\n",
    "            # obtain the input vectors, only used when word embedding is fixed (not trained end-to-end)\n",
    "            arr = np.array(hidden_state, dtype=np.float32)\n",
    "\n",
    "            out_kwargs = {}\n",
    "            out_kwargs['input_ids'] = np.array(self.text_datasets['train'][idx]['input_ids'])\n",
    "            out_kwargs['input_mask'] = np.array(self.text_datasets['train'][idx]['input_mask'])\n",
    "\n",
    "            return arr, out_kwargs\n",
    "\n",
    "def _collate_batch_helper(examples, pad_token_id, max_length, return_mask=False):\n",
    "    result = torch.full([len(examples), max_length], pad_token_id, dtype=torch.int64).tolist()\n",
    "    mask_ = torch.full([len(examples), max_length], pad_token_id, dtype=torch.int64).tolist()\n",
    "    for i, example in enumerate(examples):\n",
    "        curr_len = min(len(example), max_length)\n",
    "        result[i][:curr_len] = example[:curr_len]\n",
    "        mask_[i][:curr_len] = [1] * curr_len\n",
    "    if return_mask:\n",
    "        return result, mask_\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "122a8aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = dist_util.dev()\n",
    "\n",
    "def get_named_beta_schedule(schedule_name, num_diffusion_timesteps):\n",
    "    \"\"\"\n",
    "    Get a pre-defined beta schedule for the given name.\n",
    "\n",
    "    The beta schedule library consists of beta schedules which remain similar\n",
    "    in the limit of num_diffusion_timesteps.\n",
    "    Beta schedules may be added, but should not be removed or changed once\n",
    "    they are committed to maintain backwards compatibility.\n",
    "    \"\"\"\n",
    "    if schedule_name == \"linear\":\n",
    "        # Linear schedule from Ho et al, extended to work for any number of\n",
    "        # diffusion steps.\n",
    "        scale = 1000 / num_diffusion_timesteps\n",
    "        beta_start = scale * 0.0001\n",
    "        beta_end = scale * 0.02\n",
    "        return np.linspace(\n",
    "            beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64\n",
    "        )\n",
    "    elif schedule_name == \"cosine\":\n",
    "        return betas_for_alpha_bar(\n",
    "            num_diffusion_timesteps,\n",
    "            lambda t: math.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2,\n",
    "        )\n",
    "    elif schedule_name == 'sqrt':\n",
    "        return betas_for_alpha_bar(\n",
    "            num_diffusion_timesteps,\n",
    "            lambda t: 1-np.sqrt(t + 0.0001),\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(f\"unknown beta schedule: {schedule_name}\")\n",
    "\n",
    "def betas_for_alpha_bar(num_diffusion_timesteps, alpha_bar, max_beta=0.999):\n",
    "    \"\"\"\n",
    "    Create a beta schedule that discretizes the given alpha_t_bar function,\n",
    "    which defines the cumulative product of (1-beta) over time from t = [0,1].\n",
    "\n",
    "    :param num_diffusion_timesteps: the number of betas to produce.\n",
    "    :param alpha_bar: a lambda that takes an argument t from 0 to 1 and\n",
    "                      produces the cumulative product of (1-beta) up to that\n",
    "                      part of the diffusion process.\n",
    "    :param max_beta: the maximum beta to use; use values lower than 1 to\n",
    "                     prevent singularities.\n",
    "    \"\"\"\n",
    "    betas = []\n",
    "    for i in range(num_diffusion_timesteps):\n",
    "        t1 = i / num_diffusion_timesteps\n",
    "        t2 = (i + 1) / num_diffusion_timesteps\n",
    "        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n",
    "    return np.array(betas)\n",
    "\n",
    "class GaussianDiffusion:\n",
    "    \"\"\"\n",
    "    Utilities for training and sampling diffusion models.\n",
    "\n",
    "    Ported directly from here, and then adapted over time to further experimentation.\n",
    "    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/diffusion_utils_2.py#L42\n",
    "\n",
    "    :param betas: a 1-D numpy array of betas for each diffusion timestep,\n",
    "                  starting at T and going to 1.\n",
    "    :param predict_xstart: the model outputs to predict x_0, else to predict eps.\n",
    "    :param rescale_timesteps: if True, pass floating point timesteps into the\n",
    "                              model so that they are always scaled like in the\n",
    "                              original paper (0 to 1000).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        betas,\n",
    "        predict_xstart,\n",
    "        rescale_timesteps=False,\n",
    "        device=device\n",
    "    ):\n",
    "        self.rescale_timesteps = rescale_timesteps\n",
    "        self.predict_xstart = predict_xstart\n",
    "        self.device = device\n",
    "\n",
    "        # Use float64 for accuracy.\n",
    "        betas = np.array(betas, dtype=np.float64)\n",
    "        self.betas = betas\n",
    "        assert len(betas.shape) == 1, \"betas must be 1-D\"\n",
    "        assert (betas > 0).all() and (betas <= 1).all()\n",
    "\n",
    "        self.num_timesteps = int(betas.shape[0])\n",
    "\n",
    "        alphas = 1.0 - betas\n",
    "        self.alphas_cumprod = np.cumprod(alphas, axis=0)\n",
    "        self.alphas_cumprod_prev = np.append(1.0, self.alphas_cumprod[:-1])\n",
    "        self.alphas_cumprod_next = np.append(self.alphas_cumprod[1:], 0.0)\n",
    "        assert self.alphas_cumprod_prev.shape == (self.num_timesteps,)\n",
    "\n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "        self.sqrt_alphas_cumprod = np.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - self.alphas_cumprod)\n",
    "        self.log_one_minus_alphas_cumprod = np.log(1.0 - self.alphas_cumprod)\n",
    "        self.sqrt_recip_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod)\n",
    "        self.sqrt_recipm1_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod - 1)\n",
    "\n",
    "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "        self.posterior_variance = (\n",
    "            betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        # log calculation clipped because the posterior variance is 0 at the\n",
    "        # beginning of the diffusion chain.\n",
    "        self.posterior_log_variance_clipped = np.log(\n",
    "            np.append(self.posterior_variance[1], self.posterior_variance[1:])\n",
    "        )\n",
    "        self.posterior_mean_coef1 = (\n",
    "            betas * np.sqrt(self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        self.posterior_mean_coef2 = (\n",
    "            (1.0 - self.alphas_cumprod_prev)\n",
    "            * np.sqrt(alphas)\n",
    "            / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "\n",
    "        self.mapping_func = None # implement in train main()\n",
    "\n",
    "    def training_losses(self, model, *args, **kwargs):\n",
    "        self.model = model\n",
    "        return self.training_losses_seq2seq(model, *args, **kwargs)\n",
    "\n",
    "    def _predict_xstart_from_eps(self, x_t, t, eps):\n",
    "        assert x_t.shape == eps.shape\n",
    "        return (\n",
    "            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t\n",
    "            - _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * eps\n",
    "        )\n",
    "\n",
    "    def _predict_eps_from_xstart(self, x_t, t, pred_xstart):\n",
    "        return (\n",
    "            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t\n",
    "            - pred_xstart\n",
    "        ) / _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n",
    "\n",
    "    def _scale_timesteps(self, t):\n",
    "        if self.rescale_timesteps:\n",
    "            return t.float() * (1000.0 / self.num_timesteps)\n",
    "        return t\n",
    "\n",
    "    def q_mean_variance(self, x_start, t):\n",
    "        \"\"\"\n",
    "        Get the distribution q(x_t | x_0).\n",
    "\n",
    "        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n",
    "        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n",
    "        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n",
    "        \"\"\"\n",
    "        mean = (\n",
    "            _extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n",
    "        )\n",
    "        variance = _extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n",
    "        log_variance = _extract_into_tensor(\n",
    "            self.log_one_minus_alphas_cumprod, t, x_start.shape\n",
    "        )\n",
    "        return mean, variance, log_variance\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None, mask=None):\n",
    "        \"\"\"\n",
    "        Diffuse the data for a given number of diffusion steps.\n",
    "\n",
    "        In other words, sample from q(x_t | x_0).\n",
    "\n",
    "        :param x_start: the initial data batch.\n",
    "        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n",
    "        :param noise: if specified, the split-out normal noise.\n",
    "        :param mask: anchoring masked position\n",
    "        :return: A noisy version of x_start.\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "\n",
    "        assert noise.shape == x_start.shape\n",
    "        x_t = (\n",
    "            _extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n",
    "            + _extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n",
    "            * noise\n",
    "        )\n",
    "\n",
    "        if mask == None:\n",
    "            return x_t\n",
    "        else:\n",
    "            mask = torch.broadcast_to(mask.unsqueeze(dim=-1), x_start.shape)\n",
    "            return torch.where(mask==0, x_start, x_t)\n",
    "\n",
    "    def q_posterior_mean_variance(self, x_start, x_t, t):\n",
    "        \"\"\"\n",
    "        Compute the mean and variance of the diffusion posterior: \n",
    "            q(x_{t-1} | x_t, x_0)\n",
    "\n",
    "        \"\"\"\n",
    "        assert x_start.shape == x_t.shape\n",
    "        posterior_mean = (\n",
    "            _extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start\n",
    "            + _extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        )\n",
    "        posterior_variance = _extract_into_tensor(self.posterior_variance, t, x_t.shape)\n",
    "        posterior_log_variance_clipped = _extract_into_tensor(\n",
    "            self.posterior_log_variance_clipped, t, x_t.shape\n",
    "        )\n",
    "        assert (\n",
    "            posterior_mean.shape[0]\n",
    "            == posterior_variance.shape[0]\n",
    "            == posterior_log_variance_clipped.shape[0]\n",
    "            == x_start.shape[0]\n",
    "        )\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "\n",
    "    def p_mean_variance(\n",
    "        self, model, x, t, clip_denoised=True, denoised_fn=None, model_kwargs=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Apply the model to get p(x_{t-1} | x_t), as well as a prediction of\n",
    "        the initial x, x_0.\n",
    "\n",
    "        :param model: the model, which takes a signal and a batch of timesteps\n",
    "                      as input.\n",
    "        :param x: the [N x C x ...] tensor at time t.\n",
    "        :param t: a 1-D Tensor of timesteps.\n",
    "        :param clip_denoised: if True, clip the denoised signal into [-1, 1].\n",
    "        :param denoised_fn: if not None, a function which applies to the\n",
    "            x_start prediction before it is used to sample. Applies before\n",
    "            clip_denoised.\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :return: a dict with the following keys:\n",
    "                 - 'mean': the model mean output.\n",
    "                 - 'variance': the model variance output.\n",
    "                 - 'log_variance': the log of 'variance'.\n",
    "                 - 'pred_xstart': the prediction for x_0.\n",
    "        \"\"\"\n",
    "        if model_kwargs is None:\n",
    "            model_kwargs = {}\n",
    "\n",
    "        B, C = x.size(0), x.size(-1)\n",
    "        assert t.shape == (B,)\n",
    "        # print(x.shape)\n",
    "        model_output = model(x, self._scale_timesteps(t), **model_kwargs)\n",
    "        \n",
    "        # for fixedlarge, we set the initial (log-)variance like so\n",
    "        # to get a better decoder log likelihood.\n",
    "        model_variance = np.append(self.posterior_variance[1], self.betas[1:])\n",
    "        model_log_variance = np.log(np.append(self.posterior_variance[1], self.betas[1:]))\n",
    "        \n",
    "        model_variance = _extract_into_tensor(model_variance, t, x.shape)\n",
    "        model_log_variance = _extract_into_tensor(model_log_variance, t, x.shape)\n",
    "\n",
    "        def process_xstart(x):\n",
    "            if denoised_fn is not None:\n",
    "                # print(denoised_fn)\n",
    "                x = denoised_fn(x, t)\n",
    "            if clip_denoised:\n",
    "                return x.clamp(-1, 1)\n",
    "            return x\n",
    "\n",
    "        if self.predict_xstart:\n",
    "            pred_xstart = process_xstart(model_output)\n",
    "        else:\n",
    "            ### model is used to predict eps\n",
    "            pred_xstart = process_xstart(\n",
    "                self._predict_xstart_from_eps(x_t=x, t=t, eps=model_output)\n",
    "            )\n",
    "\n",
    "        model_mean, _, _ = self.q_posterior_mean_variance(\n",
    "            x_start=pred_xstart, x_t=x, t=t\n",
    "        )\n",
    "\n",
    "        assert (\n",
    "            model_mean.shape == model_log_variance.shape == pred_xstart.shape == x.shape\n",
    "        )\n",
    "        return {\n",
    "            \"mean\": model_mean,\n",
    "            \"variance\": model_variance,\n",
    "            \"log_variance\": model_log_variance,\n",
    "            \"pred_xstart\": pred_xstart,\n",
    "        }\n",
    "\n",
    "    def p_sample(\n",
    "        self, model, x, t, clip_denoised=True, denoised_fn=None, model_kwargs=None,\n",
    "            top_p=None, mask=None, x_start=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sample x_{t-1} from the model at the given timestep.\n",
    "\n",
    "        :param model: the model to sample from.\n",
    "        :param x: the current tensor at x_{t-1}.\n",
    "        :param t: the value of t, starting at 0 for the first diffusion step.\n",
    "        :param clip_denoised: if True, clip the x_start prediction to [-1, 1].\n",
    "        :param denoised_fn: if not None, a function which applies to the\n",
    "            x_start prediction before it is used to sample.\n",
    "        :param mask: anchoring masked position to x_start\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :return: a dict containing the following keys:\n",
    "                 - 'sample': a random sample from the model.\n",
    "                 - 'pred_xstart': a prediction of x_0.\n",
    "        \"\"\"\n",
    "        out = self.p_mean_variance(\n",
    "            model,\n",
    "            x,\n",
    "            t,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "        )\n",
    "        if top_p is not None and top_p > 0:\n",
    "            # print('top_p sampling')\n",
    "            noise = torch.randn_like(x)\n",
    "            replace_mask = torch.abs(noise) > top_p\n",
    "            while replace_mask.any():\n",
    "                noise[replace_mask] = torch.randn_like(noise[replace_mask])\n",
    "                replace_mask = torch.abs(noise) > top_p\n",
    "            assert (torch.abs(noise) <= top_p).all()\n",
    "\n",
    "        else:\n",
    "            noise = torch.randn_like(x)\n",
    "\n",
    "        nonzero_mask = (\n",
    "            (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))\n",
    "        )  # no noise when t == 0\n",
    "        sample = out[\"mean\"] + nonzero_mask * torch.exp(0.5 * out[\"log_variance\"]) * noise\n",
    "        if mask == None:\n",
    "            pass\n",
    "        else:\n",
    "            sample = torch.where(mask==0, x_start, sample)\n",
    "\n",
    "        return {\n",
    "            \"sample\": sample, \n",
    "            \"pred_xstart\": out[\"pred_xstart\"],\n",
    "            \"greedy_mean\": out[\"mean\"], \n",
    "            \"out\": out\n",
    "        }\n",
    "\n",
    "    \n",
    "    def p_sample_loop(\n",
    "        self,\n",
    "        model,\n",
    "        shape,\n",
    "        noise=None,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        progress=True,\n",
    "        top_p=None,\n",
    "        clamp_step=None,\n",
    "        clamp_first=None,\n",
    "        mask=None,\n",
    "        x_start=None,\n",
    "        gap=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate samples from the model.\n",
    "\n",
    "        :param model: the model module.\n",
    "        :param shape: the shape of the samples, (N, C, H, W).\n",
    "        :param noise: if specified, the noise from the encoder to sample.\n",
    "                      Should be of the same shape as `shape`.\n",
    "        :param clip_denoised: if True, clip x_start predictions to [-1, 1].\n",
    "        :param denoised_fn: if not None, a function which applies to the\n",
    "            x_start prediction before it is used to sample.\n",
    "        :param mask: anchoring masked position to x_start\n",
    "        :param clamp_step: in clamp_first mode, choose end clamp step, otherwise starting clamp step\n",
    "        :param clamp_first: bool, clamp_first mode\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :param device: if specified, the device to create the samples on.\n",
    "                       If not specified, use a model parameter's device.\n",
    "        :param progress: if True, show a tqdm progress bar.\n",
    "        :return: a non-differentiable batch of samples.\n",
    "        \"\"\"\n",
    "        final = []\n",
    "        for sample in self.p_sample_loop_progressive(\n",
    "            model,\n",
    "            shape,\n",
    "            noise=noise,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "            progress=progress,\n",
    "            top_p=top_p,\n",
    "            clamp_step=clamp_step,\n",
    "            clamp_first=clamp_first,\n",
    "            mask=mask,\n",
    "            x_start=x_start\n",
    "        ):\n",
    "            final.append(sample['sample'])\n",
    "        return final\n",
    "\n",
    "    def p_sample_loop_progressive(\n",
    "        self,\n",
    "        model,\n",
    "        shape,\n",
    "        noise=None,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        progress=True,\n",
    "        top_p=None,\n",
    "        clamp_step=None,\n",
    "        clamp_first=None,\n",
    "        mask=None,\n",
    "        x_start=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate samples from the model and yield intermediate samples from\n",
    "        each timestep of diffusion.\n",
    "\n",
    "        Arguments are the same as p_sample_loop().\n",
    "        Returns a generator over dicts, where each dict is the return value of\n",
    "        p_sample().\n",
    "        \"\"\"\n",
    "        assert isinstance(shape, (tuple, list))\n",
    "        if noise is not None: # custom your the start point of x_0\n",
    "            sample_x = noise\n",
    "        else:\n",
    "            sample_x = torch.randn(*shape, device=self.device)\n",
    "        indices = list(range(self.num_timesteps))[::-1]\n",
    "\n",
    "        if progress:\n",
    "            # Lazy import so that we don't depend on tqdm.\n",
    "            from tqdm.auto import tqdm\n",
    "            indices = tqdm(indices)\n",
    "\n",
    "        for i in indices: # from T to 0\n",
    "            t = torch.tensor([i] * shape[0], device=self.device)\n",
    "            if not clamp_first:\n",
    "                if i > clamp_step:\n",
    "                    denoised_fn_cur = None\n",
    "                else:\n",
    "                    denoised_fn_cur = denoised_fn\n",
    "            else:\n",
    "                if i >= clamp_step:\n",
    "                    denoised_fn_cur = denoised_fn\n",
    "                else:\n",
    "                    denoised_fn_cur = None\n",
    "            with torch.no_grad():\n",
    "                out = self.p_sample(\n",
    "                    model,\n",
    "                    sample_x,\n",
    "                    t,\n",
    "                    clip_denoised=clip_denoised,\n",
    "                    denoised_fn=denoised_fn_cur,\n",
    "                    model_kwargs=model_kwargs,\n",
    "                    top_p=top_p,\n",
    "                    mask=mask,\n",
    "                    x_start=x_start\n",
    "                )\n",
    "                yield out\n",
    "                sample_x = out[\"sample\"]\n",
    "\n",
    "\n",
    "    def _get_x_start(self, x_start_mean, std):\n",
    "        '''\n",
    "        Word embedding projection from {Emb(w)} to {x_0}\n",
    "        :param x_start_mean: word embedding\n",
    "        :return: x_0\n",
    "        '''\n",
    "        noise = torch.randn_like(x_start_mean)\n",
    "        assert noise.shape == x_start_mean.shape\n",
    "        # print(x_start_mean.device, noise.device)\n",
    "        return (\n",
    "             x_start_mean + std * noise\n",
    "        )\n",
    "\n",
    "    def _token_discrete_loss(self, x_t, get_logits, input_ids, mask=None, truncate=False, t=None):\n",
    "        '''\n",
    "        the loss of -log p(w|z_0)\n",
    "        :param x_start_mean: word embedding\n",
    "        :return: x_0\n",
    "        '''\n",
    "        reshaped_x_t = x_t\n",
    "        logits = get_logits(reshaped_x_t)  # bsz, seqlen, vocab\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        decoder_nll = loss_fct(logits.view(-1, logits.size(-1)), input_ids.view(-1)).view(input_ids.shape)\n",
    "        if mask != None:\n",
    "            decoder_nll *= mask\n",
    "        # print(decoder_nll.shape)\n",
    "        if mask != None:\n",
    "            decoder_nll = decoder_nll.sum(dim=-1)/mask.sum(dim=-1)\n",
    "        else:\n",
    "            decoder_nll = decoder_nll.mean(dim=-1)\n",
    "\n",
    "        return decoder_nll\n",
    "\n",
    "    def _x0_helper(self, model_output, x, t):\n",
    "\n",
    "        if self.predict_xstart:\n",
    "            pred_xstart = model_output\n",
    "            pred_prev, _, _ = self.q_posterior_mean_variance(\n",
    "                x_start=pred_xstart, x_t=x, t=t\n",
    "            )\n",
    "\n",
    "        else: # predict eps\n",
    "            pred_xstart = self._predict_xstart_from_eps(x_t=x, t=t, eps=model_output)\n",
    "        \n",
    "            pred_prev, _, _ = self.q_posterior_mean_variance(\n",
    "                x_start=pred_xstart, x_t=x, t=t\n",
    "            )\n",
    "\n",
    "        return {'pred_xprev':pred_prev, 'pred_xstart':pred_xstart}\n",
    "\n",
    "    def training_losses_seq2seq(self, model, x_start, t, model_kwargs=None, noise=None):\n",
    "        \"\"\"\n",
    "        Compute training losses for a single timestep.\n",
    "\n",
    "        :param model: the model to evaluate loss on.\n",
    "        :param x_start: the [N x C x ...] tensor of inputs. # not used unless fixing the input embeddings\n",
    "        :param t: a batch of timestep indices.\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :param noise: if specified, the specific Gaussian noise to try to remove.\n",
    "        :return: a dict with the key \"loss\" containing a tensor of shape [N].\n",
    "                 Some mean or variance settings may also have other keys.\n",
    "        \"\"\"\n",
    "        x_start_fix = x_start # save the orignal x_0\n",
    "        assert 'input_ids' in model_kwargs\n",
    "        input_ids_x = model_kwargs.pop('input_ids').to(self.device)\n",
    "        input_ids_mask = model_kwargs.pop('input_mask').to(self.device)\n",
    "        x_start_mean = model.model.get_embeds(input_ids_x).to(self.device)\n",
    "                \n",
    "        std = _extract_into_tensor(self.sqrt_one_minus_alphas_cumprod,\n",
    "                                   torch.tensor([0]),\n",
    "                                   x_start_mean.shape)\n",
    "        # print(std.shape, )\n",
    "        x_start = self._get_x_start(x_start_mean, std)\n",
    "#         print(\"x_start_mean shape: \", x_start_mean.shape, \"x_start shape: \", x_start.shape)\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "\n",
    "        # add noise\n",
    "        x_t = self.q_sample(x_start, t, noise=noise, mask=input_ids_mask) # reparametrization trick.\n",
    "\n",
    "        get_logits = model.model.get_logits\n",
    "\n",
    "        terms = {}\n",
    "\n",
    "        target = x_start\n",
    "        model_output = model(x_t, self._scale_timesteps(t), **model_kwargs)\n",
    "        assert model_output.shape == target.shape == x_start.shape\n",
    "        terms[\"mse\"] = mean_flat((target - model_output) ** 2)\n",
    "\n",
    "        model_out_x_start = self._x0_helper(model_output, x_t, t)['pred_xstart'] # predicted_xstart = model_output\n",
    "        t0_mask = (t == 0)\n",
    "        t0_loss = mean_flat((x_start_mean - model_out_x_start) ** 2)\n",
    "        terms[\"mse\"] = torch.where(t0_mask, t0_loss, terms[\"mse\"])\n",
    "\n",
    "        # tT_mask = (t == self.num_timesteps - 1)\n",
    "        out_mean, _, _ = self.q_mean_variance(x_start, torch.LongTensor([self.num_timesteps - 1]).to(self.device))\n",
    "        tT_loss =  mean_flat(out_mean ** 2)\n",
    "\n",
    "        decoder_nll = self._token_discrete_loss(x_start, get_logits, input_ids_x) # embedding regularization\n",
    "        terms[\"nll\"] = self._token_discrete_loss(model_out_x_start, get_logits, input_ids_x, mask=input_ids_mask, truncate=True, t=t) # x_0->model_out_x_start\n",
    "        # assert (model.lm_head.weight == model.word_embedding.weight).all()\n",
    "\n",
    "        terms[\"loss\"] = terms[\"mse\"] + decoder_nll + tT_loss\n",
    "\n",
    "        return terms\n",
    "\n",
    "    def ddim_sample(\n",
    "        self,\n",
    "        model,\n",
    "        x,\n",
    "        t,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        eta=0.0,\n",
    "        langevin_fn=None,\n",
    "        mask=None,\n",
    "        x_start=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sample x_{t-1} from the model using DDIM.\n",
    "\n",
    "        Same usage as p_sample().\n",
    "        \"\"\"\n",
    "        out = self.p_mean_variance(\n",
    "            model,\n",
    "            x,\n",
    "            t,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "        )\n",
    "        # Usually our model outputs epsilon, but we re-derive it\n",
    "        # in case we used x_start or x_prev prediction.\n",
    "        eps = self._predict_eps_from_xstart(x, t, out[\"pred_xstart\"])\n",
    "        alpha_bar = _extract_into_tensor(self.alphas_cumprod, t, x.shape)\n",
    "        alpha_bar_prev = _extract_into_tensor(self.alphas_cumprod_prev, t, x.shape)\n",
    "        sigma = (\n",
    "            eta\n",
    "            * torch.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar))\n",
    "            * torch.sqrt(1 - alpha_bar / alpha_bar_prev)\n",
    "        )\n",
    "        # Equation 12.\n",
    "        noise = torch.randn_like(x)\n",
    "        mean_pred = (\n",
    "            out[\"pred_xstart\"] * torch.sqrt(alpha_bar_prev)\n",
    "            + torch.sqrt(1 - alpha_bar_prev - sigma ** 2) * eps\n",
    "        )\n",
    "        nonzero_mask = (\n",
    "            (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))\n",
    "        )  # no noise when t == 0\n",
    "        # print(sigma.mean())\n",
    "        sample = mean_pred + nonzero_mask * sigma * noise\n",
    "        \n",
    "        if mask == None:\n",
    "            pass\n",
    "        else:\n",
    "            sample = torch.where(mask==0, x_start, sample)\n",
    "        \n",
    "        return {\"sample\": sample, \"pred_xstart\": out[\"pred_xstart\"]}\n",
    "\n",
    "    def ddim_sample_loop(\n",
    "        self,\n",
    "        model,\n",
    "        shape,\n",
    "        noise=None,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        progress=True,\n",
    "        top_p=None,\n",
    "        clamp_step=None,\n",
    "        clamp_first=None,\n",
    "        mask=None,\n",
    "        x_start=None,\n",
    "        gap=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate samples from the model using DDIM.\n",
    "        :param gap: compute ddim sampling for each {gap} step\n",
    "\n",
    "        Same usage as p_sample_loop().\n",
    "        \"\"\"\n",
    "        final = []\n",
    "        for sample in self.ddim_sample_loop_progressive(\n",
    "            model,\n",
    "            shape,\n",
    "            noise=noise,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "            progress=progress,\n",
    "            mask=mask,\n",
    "            x_start=x_start,\n",
    "            gap = gap\n",
    "        ):\n",
    "            final.append(sample['sample'])\n",
    "        return final\n",
    "\n",
    "    def ddim_sample_loop_progressive(\n",
    "        self,\n",
    "        model,\n",
    "        shape,\n",
    "        noise=None,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        progress=True,\n",
    "        eta=0.0,\n",
    "        langevin_fn=None,\n",
    "        mask=None,\n",
    "        x_start=None,\n",
    "        gap=1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Use DDIM to sample from the model and yield intermediate samples from\n",
    "        each timestep of DDIM.\n",
    "\n",
    "        Same usage as p_sample_loop_progressive().\n",
    "        \"\"\"\n",
    "        assert isinstance(shape, (tuple, list))\n",
    "        if noise is not None:\n",
    "            sample_x = noise\n",
    "        else:\n",
    "            sample_x = torch.randn(*shape, device=self.device)\n",
    "        indices = list(range(self.num_timesteps))[::-1][::gap]\n",
    "\n",
    "        if progress:\n",
    "            # Lazy import so that we don't depend on tqdm.\n",
    "            from tqdm.auto import tqdm\n",
    "\n",
    "            indices = tqdm(indices)\n",
    "\n",
    "        for i in indices:\n",
    "            t = torch.tensor([i] * shape[0], device=self.device)\n",
    "            with torch.no_grad():\n",
    "                out = self.ddim_sample(\n",
    "                    model,\n",
    "                    sample_x,\n",
    "                    t,\n",
    "                    clip_denoised=clip_denoised,\n",
    "                    denoised_fn=denoised_fn,\n",
    "                    model_kwargs=model_kwargs,\n",
    "                    mask=mask,\n",
    "                    x_start=x_start\n",
    "                )\n",
    "                yield out\n",
    "                sample_x = out[\"sample\"]\n",
    "\n",
    "def _extract_into_tensor(arr, timesteps, broadcast_shape):\n",
    "    \"\"\"\n",
    "    Extract values from a 1-D numpy array for a batch of indices.\n",
    "\n",
    "    :param arr: the 1-D numpy array.\n",
    "    :param timesteps: a tensor of indices into the array to extract.\n",
    "    :param broadcast_shape: a larger shape of K dimensions with the batch\n",
    "                            dimension equal to the length of timesteps.\n",
    "    :return: a tensor of shape [batch_size, 1, ...] where the shape has K dims.\n",
    "    \"\"\"\n",
    "    res = torch.from_numpy(arr).to(device)[timesteps].float()\n",
    "    while len(res.shape) < len(broadcast_shape):\n",
    "        res = res[..., None]\n",
    "    return res.expand(broadcast_shape)\n",
    "\n",
    "class SpacedDiffusion(GaussianDiffusion):\n",
    "    \"\"\"\n",
    "    A diffusion process which can skip steps in a base diffusion process.\n",
    "    :param kwargs: the kwargs to create the base diffusion process.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.timestep_map = []\n",
    "        self.original_num_steps = len(kwargs[\"betas\"])\n",
    "\n",
    "        # print(kwargs.keys())\n",
    "        base_diffusion = GaussianDiffusion(**kwargs)  # pylint: disable=missing-kwoa\n",
    "        last_alpha_cumprod = 1.0\n",
    "        new_betas = []\n",
    "        for i, alpha_cumprod in enumerate(base_diffusion.alphas_cumprod):\n",
    "            new_betas.append(1 - alpha_cumprod / last_alpha_cumprod)\n",
    "            last_alpha_cumprod = alpha_cumprod\n",
    "            self.timestep_map.append(i)\n",
    "        kwargs[\"betas\"] = np.array(new_betas)\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def p_mean_variance(\n",
    "        self, model, *args, **kwargs\n",
    "    ):  # pylint: disable=signature-differs\n",
    "        # print('called p_mean_var')\n",
    "        return super().p_mean_variance(self._wrap_model(model), *args, **kwargs)\n",
    "\n",
    "    def training_losses(\n",
    "        self, model, *args, **kwargs\n",
    "    ):  # pylint: disable=signature-differs\n",
    "        # print('called training_losses')\n",
    "        return super().training_losses(self._wrap_model(model), *args, **kwargs)\n",
    "\n",
    "    def _wrap_model(self, model):\n",
    "        if isinstance(model, _WrappedModel):\n",
    "            return model\n",
    "        return _WrappedModel(\n",
    "            model, self.timestep_map, self.rescale_timesteps, self.original_num_steps\n",
    "        )\n",
    "\n",
    "    def _scale_timesteps(self, t):\n",
    "        # Scaling is done by the wrapped model.\n",
    "        return t\n",
    "\n",
    "\n",
    "class _WrappedModel:\n",
    "    def __init__(self, model, timestep_map, rescale_timesteps, original_num_steps, device=device):\n",
    "        self.model = model\n",
    "        self.timestep_map = timestep_map\n",
    "        self.rescale_timesteps = rescale_timesteps\n",
    "        self.original_num_steps = original_num_steps\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, x, ts, **kwargs):\n",
    "        # print(ts)\n",
    "        map_tensor = torch.tensor(self.timestep_map, device=self.device, dtype=ts.dtype)\n",
    "        new_ts = map_tensor[ts]\n",
    "        # print(new_ts)\n",
    "        if self.rescale_timesteps:\n",
    "            new_ts = new_ts.float() * (1000.0 / self.original_num_steps)\n",
    "        return self.model(x, new_ts, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0e2e2bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.nn import mean_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f0394b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "# from transformers import BertEncoder\n",
    "from transformers.models.bert.modeling_bert import BertEncoder, BertModel\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils.nn import (\n",
    "    SiLU,\n",
    "    linear,\n",
    "    timestep_embedding,\n",
    ")\n",
    "\n",
    "class TransformerNetModel(nn.Module):\n",
    "    \"\"\"\n",
    "    The full Transformer model with attention and timestep embedding.\n",
    "\n",
    "    :param input_dims: dims of the input Tensor.\n",
    "    :param output_dims: dims of the output Tensor.\n",
    "    :param hidden_t_dim: dims of time embedding.\n",
    "    :param dropout: the dropout probability.\n",
    "    :param config/config_name: thew config of PLMs.\n",
    "    :param init_pretrained: bool, init whole network params with PLMs.\n",
    "    :param vocab_size: the size of vocabulary\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dims,\n",
    "        output_dims,\n",
    "        hidden_t_dim,\n",
    "        dropout=0,\n",
    "        config=None,\n",
    "        config_name='bert-base-uncased',\n",
    "        vocab_size=None,\n",
    "        init_pretrained='no',\n",
    "        logits_mode=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if config is None:\n",
    "            config = AutoConfig.from_pretrained(config_name)\n",
    "            config.hidden_dropout_prob = dropout\n",
    "\n",
    "        self.input_dims = input_dims\n",
    "        self.hidden_t_dim = hidden_t_dim\n",
    "        self.output_dims = output_dims\n",
    "        self.dropout = dropout\n",
    "        self.logits_mode = logits_mode\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.word_embedding = nn.Embedding(vocab_size, self.input_dims)\n",
    "        self.lm_head = nn.Linear(self.input_dims, vocab_size)\n",
    "        with torch.no_grad():\n",
    "            self.lm_head.weight = self.word_embedding.weight\n",
    "\n",
    "        time_embed_dim = hidden_t_dim * 4\n",
    "        self.time_embed = nn.Sequential(\n",
    "            linear(hidden_t_dim, time_embed_dim),\n",
    "            SiLU(),\n",
    "            linear(time_embed_dim, config.hidden_size),\n",
    "        )\n",
    "\n",
    "        if self.input_dims != config.hidden_size:\n",
    "            self.input_up_proj = nn.Sequential(nn.Linear(input_dims, config.hidden_size),\n",
    "                                              nn.Tanh(), nn.Linear(config.hidden_size, config.hidden_size))\n",
    "        \n",
    "        if init_pretrained == 'bert':\n",
    "            print('initializing from pretrained bert...')\n",
    "            print(config)\n",
    "            temp_bert = BertModel.from_pretrained(config_name, config=config)\n",
    "\n",
    "            self.word_embedding = temp_bert.embeddings.word_embeddings\n",
    "            with torch.no_grad():\n",
    "                self.lm_head.weight = self.word_embedding.weight\n",
    "            # self.lm_head.weight.requires_grad = False\n",
    "            # self.word_embedding.weight.requires_grad = False\n",
    "            \n",
    "            self.input_transformers = temp_bert.encoder\n",
    "            self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "            self.position_embeddings = temp_bert.embeddings.position_embeddings\n",
    "            self.LayerNorm = temp_bert.embeddings.LayerNorm\n",
    "\n",
    "            del temp_bert.embeddings\n",
    "            del temp_bert.pooler\n",
    "\n",
    "        elif init_pretrained == 'no':\n",
    "            self.input_transformers = BertEncoder(config)\n",
    "\n",
    "            self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "            self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "            self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        \n",
    "        else:\n",
    "            assert False, \"invalid type of init_pretrained\"\n",
    "        \n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        if self.output_dims != config.hidden_size:\n",
    "            self.output_down_proj = nn.Sequential(nn.Linear(config.hidden_size, config.hidden_size),\n",
    "                                                nn.Tanh(), nn.Linear(config.hidden_size, self.output_dims))\n",
    "\n",
    "    def get_embeds(self, input_ids):\n",
    "        return self.word_embedding(input_ids)\n",
    "\n",
    "    def get_logits(self, hidden_repr):\n",
    "        if self.logits_mode == 1:\n",
    "            return self.lm_head(hidden_repr)\n",
    "        elif self.logits_mode == 2: # standard cosine similarity\n",
    "            text_emb = hidden_repr\n",
    "            emb_norm = (self.lm_head.weight ** 2).sum(-1).view(-1, 1)  # vocab\n",
    "            text_emb_t = torch.transpose(text_emb.view(-1, text_emb.size(-1)), 0, 1)  # d, bsz*seqlen\n",
    "            arr_norm = (text_emb ** 2).sum(-1).view(-1, 1)  # bsz*seqlen, 1\n",
    "            dist = emb_norm + arr_norm.transpose(0, 1) - 2.0 * torch.mm(self.lm_head.weight,\n",
    "                                                                     text_emb_t)  # (vocab, d) x (d, bsz*seqlen)\n",
    "            scores = torch.sqrt(torch.clamp(dist, 0.0, np.inf)).view(emb_norm.size(0), hidden_repr.size(0),\n",
    "                                                               hidden_repr.size(1)) # vocab, bsz*seqlen\n",
    "            scores = -scores.permute(1, 2, 0).contiguous()\n",
    "            return scores\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "    def forward(self, x, timesteps):\n",
    "        \"\"\"\n",
    "        Apply the model to an input batch.\n",
    "\n",
    "        :param x: an [N x C x ...] Tensor of inputs.\n",
    "        :param timesteps: a 1-D batch of timesteps.\n",
    "        :return: an [N x C x ...] Tensor of outputs.\n",
    "        \"\"\"\n",
    "        emb_t = self.time_embed(timestep_embedding(timesteps, self.hidden_t_dim))\n",
    "\n",
    "        if self.input_dims != self.hidden_size:\n",
    "            emb_x = self.input_up_proj(x)\n",
    "        else:\n",
    "            emb_x = x\n",
    "\n",
    "        seq_length = x.size(1)\n",
    "        position_ids = self.position_ids[:, : seq_length ]\n",
    "        # print(emb_x.shape, emb_t.shape, self.position_embeddings)\n",
    "        emb_inputs = self.position_embeddings(position_ids) + emb_x + emb_t.unsqueeze(1).expand(-1, seq_length, -1)\n",
    "        emb_inputs = self.dropout(self.LayerNorm(emb_inputs))\n",
    "\n",
    "        input_trans_hidden_states = self.input_transformers(emb_inputs).last_hidden_state\n",
    "        \n",
    "        if self.output_dims != self.hidden_size:\n",
    "            h = self.output_down_proj(input_trans_hidden_states)\n",
    "        else:\n",
    "            h = input_trans_hidden_states\n",
    "        h = h.type(x.dtype)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1f9b25a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_and_diffusion(\n",
    "    hidden_t_dim,\n",
    "    hidden_dim,\n",
    "    vocab_size,\n",
    "    config_name,\n",
    "    use_plm_init,\n",
    "    dropout,\n",
    "    diffusion_steps,\n",
    "    noise_schedule,\n",
    "    predict_xstart,\n",
    "    rescale_timesteps,\n",
    "    **kwargs,\n",
    "):\n",
    "    model = TransformerNetModel(\n",
    "        input_dims=hidden_dim,\n",
    "        output_dims=hidden_dim,\n",
    "        hidden_t_dim=hidden_t_dim,\n",
    "        dropout=dropout,\n",
    "        config_name=config_name,\n",
    "        vocab_size=vocab_size,\n",
    "        init_pretrained=use_plm_init\n",
    "    )\n",
    "\n",
    "    betas = get_named_beta_schedule(noise_schedule, diffusion_steps)\n",
    "\n",
    "    diffusion = SpacedDiffusion(\n",
    "        betas=betas,\n",
    "        rescale_timesteps=rescale_timesteps,\n",
    "        predict_xstart=predict_xstart,\n",
    "    )\n",
    "\n",
    "    return model, diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f06dee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer('custom', config_name, custom_vocab_fp=None, custom_vocab_list=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dbd3fbfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='', vocab_size=27829, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1701606f-3211-4741-a7f4-bc4ee146c4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 605, 137, 26, 408, 119, 2971, 645, 93, 7012, 3]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_token('find we a time for fright peace to pant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3256f999",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weight, tokenizer = load_model_emb(hidden_dim, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5366e001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(27829, 64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2542d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "## very very important to set this!!!!!\n",
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "45543e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27829"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5cc4920c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## \n",
      "Loading text data...\n",
      "############################## \n",
      "Loading dataset from data...\n",
      "### Loading form the TRAIN set...\n",
      "### Data samples...\n",
      " ['so shaken as we are so wan with care', 'find we a time for fright peace to pant'] ['find we a time for fright peace to pant', 'and breathe short-wind accents of new broils']\n",
      "RAM used: 4124.20 MB\n",
      "This is raw_datasets:  Dataset({\n",
      "    features: ['src', 'trg'],\n",
      "    num_rows: 110838\n",
      "})\n",
      "RAM used: 4142.39 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf1f7c03b8a4900b72068331a856def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=4):   0%|          | 0/110838 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### tokenized_datasets Dataset({\n",
      "    features: ['input_id_x', 'input_id_y'],\n",
      "    num_rows: 110838\n",
      "})\n",
      "### tokenized_datasets...example [2, 150, 12941, 142, 137, 205, 150, 7309, 130, 1015, 3]\n",
      "RAM used: 4247.21 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b9bff3db0b473c8adff7e24fc362bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merge and mask:   0%|          | 0/110838 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 4288.32 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be62feafc8c24c2f8c6ac74a748a6cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "padding:   0%|          | 0/110838 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "    num_rows: 110838\n",
      "}) padded dataset\n",
      "RAM used: 4473.66 MB\n",
      "RAM used: 4473.66 MB\n"
     ]
    }
   ],
   "source": [
    "data = load_data_text(\n",
    "        batch_size=batch_size,\n",
    "        seq_len=seq_len,\n",
    "        data_dir=data_dir,\n",
    "        loaded_vocab=tokenizer,\n",
    "        model_emb=model_weight # use model's weights as init\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2749334",
   "metadata": {},
   "source": [
    "Passed in as batch in TrainLoop - this is the batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "02903fcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# next(data)[0].shape # batch_size, seq_len, hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0f5f7c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(data)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964d608d",
   "metadata": {},
   "source": [
    "Passed in as cond in TrainLoop - this is a dictionary of input_ids and input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "44f6e7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(data)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4c1d8eb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# next(data)[1]['input_ids'].shape # batch_size, hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "39c90139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(data)[1]['input_mask'].shape # batch_size, hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "85a49540",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, diffusion = create_model_and_diffusion(\n",
    "                        hidden_t_dim,\n",
    "                        hidden_dim,\n",
    "                        vocab_size,\n",
    "                        config_name,\n",
    "                        use_plm_init,\n",
    "                        dropout,\n",
    "                        diffusion_steps,\n",
    "                        noise_schedule,\n",
    "                        predict_xstart,\n",
    "                        rescale_timesteps,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4b4bbaf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerNetModel(\n",
       "  (word_embedding): Embedding(27829, 64)\n",
       "  (lm_head): Linear(in_features=64, out_features=27829, bias=True)\n",
       "  (time_embed): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=512, out_features=768, bias=True)\n",
       "  )\n",
       "  (input_up_proj): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=768, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (input_transformers): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (output_down_proj): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=768, out_features=64, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(dist_util.dev())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5a953179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27829, 64)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size, hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c1ae1a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_util.dev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9124ba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "80784a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88998453"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ee9ad85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule_sampler = create_named_schedule_sampler('uniform', diffusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bcb1caf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<step_sample.UniformSampler at 0x7fefef1a8070>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schedule_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "47493837",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fbe31a79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 1.095434546470642\n",
      "Epoch 0 Loss: 1.0956522226333618\n",
      "Epoch 0 Loss: 1.100783348083496\n",
      "Epoch 0 Loss: 1.0956400632858276\n",
      "Epoch 1 Loss: 0.5506831407546997\n",
      "Epoch 1 Loss: 0.5595839619636536\n",
      "Epoch 1 Loss: 0.5745023488998413\n",
      "Epoch 1 Loss: 0.5546802878379822\n",
      "Epoch 2 Loss: 0.4530147612094879\n",
      "Epoch 2 Loss: 0.4516162574291229\n",
      "Epoch 2 Loss: 0.4370153546333313\n",
      "Epoch 2 Loss: 0.44309014081954956\n",
      "Epoch 3 Loss: 0.3923453986644745\n",
      "Epoch 3 Loss: 0.3966912031173706\n",
      "Epoch 3 Loss: 0.38730761408805847\n",
      "Epoch 3 Loss: 0.3671610951423645\n",
      "Epoch 4 Loss: 0.3858349621295929\n",
      "Epoch 4 Loss: 0.3821607828140259\n",
      "Epoch 4 Loss: 0.35517463088035583\n",
      "Epoch 4 Loss: 0.3630501627922058\n",
      "Epoch 5 Loss: 0.35602760314941406\n",
      "Epoch 5 Loss: 0.34762486815452576\n",
      "Epoch 5 Loss: 0.3591729700565338\n",
      "Epoch 5 Loss: 0.3750910758972168\n",
      "Epoch 6 Loss: 0.3464221656322479\n",
      "Epoch 6 Loss: 0.3489547669887543\n",
      "Epoch 6 Loss: 0.3212446868419647\n",
      "Epoch 6 Loss: 0.3650192320346832\n",
      "Epoch 7 Loss: 0.3734982907772064\n",
      "Epoch 7 Loss: 0.3573182225227356\n",
      "Epoch 7 Loss: 0.34896594285964966\n",
      "Epoch 7 Loss: 0.34205085039138794\n",
      "Epoch 8 Loss: 0.3566735088825226\n",
      "Epoch 8 Loss: 0.34500786662101746\n",
      "Epoch 8 Loss: 0.36619433760643005\n",
      "Epoch 8 Loss: 0.4053310453891754\n",
      "Epoch 9 Loss: 0.35644957423210144\n",
      "Epoch 9 Loss: 0.33578571677207947\n",
      "Epoch 9 Loss: 0.3665078282356262\n",
      "Epoch 9 Loss: 0.3723208010196686\n",
      "Epoch 10 Loss: 0.3956138789653778\n",
      "Epoch 10 Loss: 0.39354202151298523\n",
      "Epoch 10 Loss: 0.3731689155101776\n",
      "Epoch 10 Loss: 0.36924368143081665\n",
      "Epoch 11 Loss: 0.3952377438545227\n",
      "Epoch 11 Loss: 0.3844854533672333\n",
      "Epoch 11 Loss: 0.419041246175766\n",
      "Epoch 11 Loss: 0.4105667471885681\n",
      "Epoch 12 Loss: 0.4007357060909271\n",
      "Epoch 12 Loss: 0.35165855288505554\n",
      "Epoch 12 Loss: 0.355015367269516\n",
      "Epoch 12 Loss: 0.4532029628753662\n",
      "Epoch 13 Loss: 0.39684057235717773\n",
      "Epoch 13 Loss: 0.4093402028083801\n",
      "Epoch 13 Loss: 0.4037338197231293\n",
      "Epoch 13 Loss: 0.3710605204105377\n",
      "Epoch 14 Loss: 0.3683933913707733\n",
      "Epoch 14 Loss: 0.35395345091819763\n",
      "Epoch 14 Loss: 0.3769499659538269\n",
      "Epoch 14 Loss: 0.36458709836006165\n",
      "Epoch 15 Loss: 0.3619723618030548\n",
      "Epoch 15 Loss: 0.4007014334201813\n",
      "Epoch 15 Loss: 0.3940151631832123\n",
      "Epoch 15 Loss: 0.2996509075164795\n",
      "Epoch 16 Loss: 0.3729484975337982\n",
      "Epoch 16 Loss: 0.34802162647247314\n",
      "Epoch 16 Loss: 0.3412929177284241\n",
      "Epoch 16 Loss: 0.32547271251678467\n",
      "Epoch 17 Loss: 0.43546316027641296\n",
      "Epoch 17 Loss: 0.4337981343269348\n",
      "Epoch 17 Loss: 0.38477668166160583\n",
      "Epoch 17 Loss: 0.3825203776359558\n",
      "Epoch 18 Loss: 0.4012371003627777\n",
      "Epoch 18 Loss: 0.4124771058559418\n",
      "Epoch 18 Loss: 0.36877259612083435\n",
      "Epoch 18 Loss: 0.38741639256477356\n",
      "Epoch 19 Loss: 0.363868772983551\n",
      "Epoch 19 Loss: 0.4193437695503235\n",
      "Epoch 19 Loss: 0.3686724603176117\n",
      "Epoch 19 Loss: 0.29845526814460754\n",
      "Epoch 20 Loss: 0.38793298602104187\n",
      "Epoch 20 Loss: 0.36801281571388245\n",
      "Epoch 20 Loss: 0.3637271821498871\n",
      "Epoch 20 Loss: 0.3969844579696655\n",
      "Epoch 21 Loss: 0.3844778537750244\n",
      "Epoch 21 Loss: 0.4002908170223236\n",
      "Epoch 21 Loss: 0.40909305214881897\n",
      "Epoch 21 Loss: 0.4211418628692627\n",
      "Epoch 22 Loss: 0.3949502408504486\n",
      "Epoch 22 Loss: 0.36985039710998535\n",
      "Epoch 22 Loss: 0.3707955479621887\n",
      "Epoch 22 Loss: 0.3210487365722656\n",
      "Epoch 23 Loss: 0.3897065222263336\n",
      "Epoch 23 Loss: 0.36920759081840515\n",
      "Epoch 23 Loss: 0.40999919176101685\n",
      "Epoch 23 Loss: 0.43568146228790283\n",
      "Epoch 24 Loss: 0.33769553899765015\n",
      "Epoch 24 Loss: 0.3566044270992279\n",
      "Epoch 24 Loss: 0.37865978479385376\n",
      "Epoch 24 Loss: 0.3531333804130554\n",
      "Epoch 25 Loss: 0.38125500082969666\n",
      "Epoch 25 Loss: 0.3426845669746399\n",
      "Epoch 25 Loss: 0.3753279745578766\n",
      "Epoch 25 Loss: 0.35768094658851624\n",
      "Epoch 26 Loss: 0.3670269250869751\n",
      "Epoch 26 Loss: 0.36367279291152954\n",
      "Epoch 26 Loss: 0.35651645064353943\n",
      "Epoch 26 Loss: 0.3397381007671356\n",
      "Epoch 27 Loss: 0.3766365945339203\n",
      "Epoch 27 Loss: 0.3569572865962982\n",
      "Epoch 27 Loss: 0.3692759573459625\n",
      "Epoch 27 Loss: 0.2959437072277069\n",
      "Epoch 28 Loss: 0.3703908920288086\n",
      "Epoch 28 Loss: 0.35508960485458374\n",
      "Epoch 28 Loss: 0.3337838649749756\n",
      "Epoch 28 Loss: 0.3660927414894104\n",
      "Epoch 29 Loss: 0.3461335301399231\n",
      "Epoch 29 Loss: 0.34057673811912537\n",
      "Epoch 29 Loss: 0.33042874932289124\n",
      "Epoch 29 Loss: 0.36979442834854126\n",
      "Epoch 30 Loss: 0.3335687816143036\n",
      "Epoch 30 Loss: 0.3678426146507263\n",
      "Epoch 30 Loss: 0.34980469942092896\n",
      "Epoch 30 Loss: 0.3473854959011078\n",
      "Epoch 31 Loss: 0.3443056643009186\n",
      "Epoch 31 Loss: 0.41002875566482544\n",
      "Epoch 31 Loss: 0.40770742297172546\n",
      "Epoch 31 Loss: 0.369586706161499\n",
      "Epoch 32 Loss: 0.38478168845176697\n",
      "Epoch 32 Loss: 0.4126198887825012\n",
      "Epoch 32 Loss: 0.38281023502349854\n",
      "Epoch 32 Loss: 0.36511558294296265\n",
      "Epoch 33 Loss: 0.35348615050315857\n",
      "Epoch 33 Loss: 0.3823542296886444\n",
      "Epoch 33 Loss: 0.3939976692199707\n",
      "Epoch 33 Loss: 0.3673964738845825\n",
      "Epoch 34 Loss: 0.3710322082042694\n",
      "Epoch 34 Loss: 0.32560840249061584\n",
      "Epoch 34 Loss: 0.3688240647315979\n",
      "Epoch 34 Loss: 0.3658764362335205\n",
      "Epoch 35 Loss: 0.36852213740348816\n",
      "Epoch 35 Loss: 0.35153481364250183\n",
      "Epoch 35 Loss: 0.3347247540950775\n",
      "Epoch 35 Loss: 0.3934767246246338\n",
      "Epoch 36 Loss: 0.3316642642021179\n",
      "Epoch 36 Loss: 0.3671126961708069\n",
      "Epoch 36 Loss: 0.366192102432251\n",
      "Epoch 36 Loss: 0.357000470161438\n",
      "Epoch 37 Loss: 0.3928427994251251\n",
      "Epoch 37 Loss: 0.38691583275794983\n",
      "Epoch 37 Loss: 0.38387352228164673\n",
      "Epoch 37 Loss: 0.35610026121139526\n",
      "Epoch 38 Loss: 0.3574107587337494\n",
      "Epoch 38 Loss: 0.33650946617126465\n",
      "Epoch 38 Loss: 0.3437371253967285\n",
      "Epoch 38 Loss: 0.3326706290245056\n",
      "Epoch 39 Loss: 0.3307625949382782\n",
      "Epoch 39 Loss: 0.32984766364097595\n",
      "Epoch 39 Loss: 0.3297635018825531\n",
      "Epoch 39 Loss: 0.3353894352912903\n",
      "Epoch 40 Loss: 0.3375328779220581\n",
      "Epoch 40 Loss: 0.32341665029525757\n",
      "Epoch 40 Loss: 0.34778597950935364\n",
      "Epoch 40 Loss: 0.2713015079498291\n",
      "Epoch 41 Loss: 0.32354822754859924\n",
      "Epoch 41 Loss: 0.3437400460243225\n",
      "Epoch 41 Loss: 0.33166980743408203\n",
      "Epoch 41 Loss: 0.3144272267818451\n",
      "Epoch 42 Loss: 0.29942378401756287\n",
      "Epoch 42 Loss: 0.3158137798309326\n",
      "Epoch 42 Loss: 0.32805243134498596\n",
      "Epoch 42 Loss: 0.34365540742874146\n",
      "Epoch 43 Loss: 0.3690357208251953\n",
      "Epoch 43 Loss: 0.3108449876308441\n",
      "Epoch 43 Loss: 0.28580427169799805\n",
      "Epoch 43 Loss: 0.3020840287208557\n",
      "Epoch 44 Loss: 0.2933717966079712\n",
      "Epoch 44 Loss: 0.28213223814964294\n",
      "Epoch 44 Loss: 0.29682084918022156\n",
      "Epoch 44 Loss: 0.4078773856163025\n",
      "Epoch 45 Loss: 0.32576796412467957\n",
      "Epoch 45 Loss: 0.374554306268692\n",
      "Epoch 45 Loss: 0.3496905267238617\n",
      "Epoch 45 Loss: 0.41364771127700806\n",
      "Epoch 46 Loss: 0.37200090289115906\n",
      "Epoch 46 Loss: 0.31191110610961914\n",
      "Epoch 46 Loss: 0.36305877566337585\n",
      "Epoch 46 Loss: 0.3873472809791565\n",
      "Epoch 47 Loss: 0.38444018363952637\n",
      "Epoch 47 Loss: 0.41828519105911255\n",
      "Epoch 47 Loss: 0.38881567120552063\n",
      "Epoch 47 Loss: 0.4400077164173126\n",
      "Epoch 48 Loss: 0.4423021972179413\n",
      "Epoch 48 Loss: 0.392304390668869\n",
      "Epoch 48 Loss: 0.44060611724853516\n",
      "Epoch 48 Loss: 0.4573906660079956\n",
      "Epoch 49 Loss: 0.3580310344696045\n",
      "Epoch 49 Loss: 0.3398745059967041\n",
      "Epoch 49 Loss: 0.40104469656944275\n",
      "Epoch 49 Loss: 0.37674158811569214\n",
      "Epoch 50 Loss: 0.331268846988678\n",
      "Epoch 50 Loss: 0.370234876871109\n",
      "Epoch 50 Loss: 0.3389168381690979\n",
      "Epoch 50 Loss: 0.3145933449268341\n",
      "Epoch 51 Loss: 0.3380506932735443\n",
      "Epoch 51 Loss: 0.33555498719215393\n",
      "Epoch 51 Loss: 0.32249870896339417\n",
      "Epoch 51 Loss: 0.3255777359008789\n",
      "Epoch 52 Loss: 0.34303179383277893\n",
      "Epoch 52 Loss: 0.30897435545921326\n",
      "Epoch 52 Loss: 0.3402133584022522\n",
      "Epoch 52 Loss: 0.3700140118598938\n",
      "Epoch 53 Loss: 0.355063259601593\n",
      "Epoch 53 Loss: 0.3561038076877594\n",
      "Epoch 53 Loss: 0.3476458489894867\n",
      "Epoch 53 Loss: 0.3669324219226837\n",
      "Epoch 54 Loss: 0.318094938993454\n",
      "Epoch 54 Loss: 0.3411482274532318\n",
      "Epoch 54 Loss: 0.34771573543548584\n",
      "Epoch 54 Loss: 0.31899556517601013\n",
      "Epoch 55 Loss: 0.32084375619888306\n",
      "Epoch 55 Loss: 0.34922125935554504\n",
      "Epoch 55 Loss: 0.3005000054836273\n",
      "Epoch 55 Loss: 0.33919763565063477\n",
      "Epoch 56 Loss: 0.3147362470626831\n",
      "Epoch 56 Loss: 0.2625974118709564\n",
      "Epoch 56 Loss: 0.29684337973594666\n",
      "Epoch 56 Loss: 0.3585408329963684\n",
      "Epoch 57 Loss: 0.2549797594547272\n",
      "Epoch 57 Loss: 0.2748686671257019\n",
      "Epoch 57 Loss: 0.25994211435317993\n",
      "Epoch 57 Loss: 0.2698483467102051\n",
      "Epoch 58 Loss: 0.2595749795436859\n",
      "Epoch 58 Loss: 0.24872393906116486\n",
      "Epoch 58 Loss: 0.2810492515563965\n",
      "Epoch 58 Loss: 0.24288682639598846\n",
      "Epoch 59 Loss: 0.23906829953193665\n",
      "Epoch 59 Loss: 0.2403971403837204\n",
      "Epoch 59 Loss: 0.2537664473056793\n",
      "Epoch 59 Loss: 0.20600149035453796\n",
      "Epoch 60 Loss: 0.21626877784729004\n",
      "Epoch 60 Loss: 0.22007010877132416\n",
      "Epoch 60 Loss: 0.27599000930786133\n",
      "Epoch 60 Loss: 0.26019859313964844\n",
      "Epoch 61 Loss: 0.22314777970314026\n",
      "Epoch 61 Loss: 0.25789162516593933\n",
      "Epoch 61 Loss: 0.22246168553829193\n",
      "Epoch 61 Loss: 0.23849815130233765\n",
      "Epoch 62 Loss: 0.22619731724262238\n",
      "Epoch 62 Loss: 0.2359485626220703\n",
      "Epoch 62 Loss: 0.20570774376392365\n",
      "Epoch 62 Loss: 0.20138096809387207\n",
      "Epoch 63 Loss: 0.21753843128681183\n",
      "Epoch 63 Loss: 0.20270906388759613\n",
      "Epoch 63 Loss: 0.2164737731218338\n",
      "Epoch 63 Loss: 0.2140020728111267\n",
      "Epoch 64 Loss: 0.22673912346363068\n",
      "Epoch 64 Loss: 0.2201703041791916\n",
      "Epoch 64 Loss: 0.22359870374202728\n",
      "Epoch 64 Loss: 0.23382826149463654\n",
      "Epoch 65 Loss: 0.2116847038269043\n",
      "Epoch 65 Loss: 0.20282740890979767\n",
      "Epoch 65 Loss: 0.22412371635437012\n",
      "Epoch 65 Loss: 0.19399189949035645\n",
      "Epoch 66 Loss: 0.21492500603199005\n",
      "Epoch 66 Loss: 0.20822086930274963\n",
      "Epoch 66 Loss: 0.21098271012306213\n",
      "Epoch 66 Loss: 0.2225927710533142\n",
      "Epoch 67 Loss: 0.20068781077861786\n",
      "Epoch 67 Loss: 0.21579034626483917\n",
      "Epoch 67 Loss: 0.20028150081634521\n",
      "Epoch 67 Loss: 0.2002318650484085\n",
      "Epoch 68 Loss: 0.20342059433460236\n",
      "Epoch 68 Loss: 0.2016998827457428\n",
      "Epoch 68 Loss: 0.22468502819538116\n",
      "Epoch 68 Loss: 0.24008992314338684\n",
      "Epoch 69 Loss: 0.2085060179233551\n",
      "Epoch 69 Loss: 0.2059846967458725\n",
      "Epoch 69 Loss: 0.17517021298408508\n",
      "Epoch 69 Loss: 0.22913450002670288\n",
      "Epoch 70 Loss: 0.20104436576366425\n",
      "Epoch 70 Loss: 0.20952115952968597\n",
      "Epoch 70 Loss: 0.2108488529920578\n",
      "Epoch 70 Loss: 0.18903541564941406\n",
      "Epoch 71 Loss: 0.1948811262845993\n",
      "Epoch 71 Loss: 0.2171267718076706\n",
      "Epoch 71 Loss: 0.18981792032718658\n",
      "Epoch 71 Loss: 0.1954180896282196\n",
      "Epoch 72 Loss: 0.20203609764575958\n",
      "Epoch 72 Loss: 0.20607535541057587\n",
      "Epoch 72 Loss: 0.216639906167984\n",
      "Epoch 72 Loss: 0.22021816670894623\n",
      "Epoch 73 Loss: 0.22207418084144592\n",
      "Epoch 73 Loss: 0.22731879353523254\n",
      "Epoch 73 Loss: 0.21575044095516205\n",
      "Epoch 73 Loss: 0.23909127712249756\n",
      "Epoch 74 Loss: 0.2077605277299881\n",
      "Epoch 74 Loss: 0.20897133648395538\n",
      "Epoch 74 Loss: 0.20564870536327362\n",
      "Epoch 74 Loss: 0.2198486030101776\n",
      "Epoch 75 Loss: 0.20966556668281555\n",
      "Epoch 75 Loss: 0.2144639492034912\n",
      "Epoch 75 Loss: 0.21623213589191437\n",
      "Epoch 75 Loss: 0.21176835894584656\n",
      "Epoch 76 Loss: 0.21632711589336395\n",
      "Epoch 76 Loss: 0.23472090065479279\n",
      "Epoch 76 Loss: 0.19223880767822266\n",
      "Epoch 76 Loss: 0.20399589836597443\n",
      "Epoch 77 Loss: 0.2367752343416214\n",
      "Epoch 77 Loss: 0.1906159669160843\n",
      "Epoch 77 Loss: 0.19783473014831543\n",
      "Epoch 77 Loss: 0.19591018557548523\n",
      "Epoch 78 Loss: 0.1875598281621933\n",
      "Epoch 78 Loss: 0.21178777515888214\n",
      "Epoch 78 Loss: 0.21661582589149475\n",
      "Epoch 78 Loss: 0.19867974519729614\n",
      "Epoch 79 Loss: 0.20778580009937286\n",
      "Epoch 79 Loss: 0.1903427541255951\n",
      "Epoch 79 Loss: 0.19850119948387146\n",
      "Epoch 79 Loss: 0.18404294550418854\n",
      "Epoch 80 Loss: 0.20548127591609955\n",
      "Epoch 80 Loss: 0.1891399621963501\n",
      "Epoch 80 Loss: 0.19731396436691284\n",
      "Epoch 80 Loss: 0.19851824641227722\n",
      "Epoch 81 Loss: 0.19426672160625458\n",
      "Epoch 81 Loss: 0.22313018143177032\n",
      "Epoch 81 Loss: 0.19869235157966614\n",
      "Epoch 81 Loss: 0.2156674861907959\n",
      "Epoch 82 Loss: 0.20247331261634827\n",
      "Epoch 82 Loss: 0.21022029221057892\n",
      "Epoch 82 Loss: 0.1911308318376541\n",
      "Epoch 82 Loss: 0.21950717270374298\n",
      "Epoch 83 Loss: 0.21675947308540344\n",
      "Epoch 83 Loss: 0.23127757012844086\n",
      "Epoch 83 Loss: 0.24330969154834747\n",
      "Epoch 83 Loss: 0.2295641303062439\n",
      "Epoch 84 Loss: 0.22968946397304535\n",
      "Epoch 84 Loss: 0.21173779666423798\n",
      "Epoch 84 Loss: 0.22699132561683655\n",
      "Epoch 84 Loss: 0.2546292245388031\n",
      "Epoch 85 Loss: 0.1940890997648239\n",
      "Epoch 85 Loss: 0.2298707515001297\n",
      "Epoch 85 Loss: 0.2215879261493683\n",
      "Epoch 85 Loss: 0.21077165007591248\n",
      "Epoch 86 Loss: 0.19567424058914185\n",
      "Epoch 86 Loss: 0.23007693886756897\n",
      "Epoch 86 Loss: 0.18994906544685364\n",
      "Epoch 86 Loss: 0.20776687562465668\n",
      "Epoch 87 Loss: 0.21088071167469025\n",
      "Epoch 87 Loss: 0.2006005495786667\n",
      "Epoch 87 Loss: 0.19846336543560028\n",
      "Epoch 87 Loss: 0.22086068987846375\n",
      "Epoch 88 Loss: 0.18956132233142853\n",
      "Epoch 88 Loss: 0.21546219289302826\n",
      "Epoch 88 Loss: 0.21434655785560608\n",
      "Epoch 88 Loss: 0.23315894603729248\n",
      "Epoch 89 Loss: 0.21420621871948242\n",
      "Epoch 89 Loss: 0.23220610618591309\n",
      "Epoch 89 Loss: 0.21637757122516632\n",
      "Epoch 89 Loss: 0.18184885382652283\n",
      "Epoch 90 Loss: 0.19512741267681122\n",
      "Epoch 90 Loss: 0.18924152851104736\n",
      "Epoch 90 Loss: 0.1902397722005844\n",
      "Epoch 90 Loss: 0.19830378890037537\n",
      "Epoch 91 Loss: 0.17637653648853302\n",
      "Epoch 91 Loss: 0.20158611238002777\n",
      "Epoch 91 Loss: 0.22088785469532013\n",
      "Epoch 91 Loss: 0.21418477594852448\n",
      "Epoch 92 Loss: 0.18578851222991943\n",
      "Epoch 92 Loss: 0.2042657434940338\n",
      "Epoch 92 Loss: 0.21416492760181427\n",
      "Epoch 92 Loss: 0.14939498901367188\n",
      "Epoch 93 Loss: 0.1897602081298828\n",
      "Epoch 93 Loss: 0.18501849472522736\n",
      "Epoch 93 Loss: 0.18788276612758636\n",
      "Epoch 93 Loss: 0.18327024579048157\n",
      "Epoch 94 Loss: 0.194989413022995\n",
      "Epoch 94 Loss: 0.19045831263065338\n",
      "Epoch 94 Loss: 0.20242753624916077\n",
      "Epoch 94 Loss: 0.17939257621765137\n",
      "Epoch 95 Loss: 0.19903896749019623\n",
      "Epoch 95 Loss: 0.1906893104314804\n",
      "Epoch 95 Loss: 0.1955205649137497\n",
      "Epoch 95 Loss: 0.1973181515932083\n",
      "Epoch 96 Loss: 0.21963536739349365\n",
      "Epoch 96 Loss: 0.1957332193851471\n",
      "Epoch 96 Loss: 0.19489090144634247\n",
      "Epoch 96 Loss: 0.1556166708469391\n",
      "Epoch 97 Loss: 0.20938900113105774\n",
      "Epoch 97 Loss: 0.19218270480632782\n",
      "Epoch 97 Loss: 0.19719350337982178\n",
      "Epoch 97 Loss: 0.20147734880447388\n",
      "Epoch 98 Loss: 0.1983872950077057\n",
      "Epoch 98 Loss: 0.2014787197113037\n",
      "Epoch 98 Loss: 0.18767867982387543\n",
      "Epoch 98 Loss: 0.20763348042964935\n",
      "Epoch 99 Loss: 0.19420969486236572\n",
      "Epoch 99 Loss: 0.21272997558116913\n",
      "Epoch 99 Loss: 0.21496033668518066\n",
      "Epoch 99 Loss: 0.22411242127418518\n"
     ]
    }
   ],
   "source": [
    "TrainLoop(\n",
    "        model=model,\n",
    "        diffusion=diffusion,\n",
    "        data=data,\n",
    "        batch_size=batch_size,\n",
    "        microbatch=microbatch,\n",
    "        lr=lr,\n",
    "        ema_rate=ema_rate,\n",
    "        schedule_sampler=schedule_sampler,\n",
    "        weight_decay=weight_decay,\n",
    "        epochs=epochs,\n",
    "#         eval_data=data_valid,\n",
    "        eval_interval=eval_interval\n",
    "    ).run_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee01293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe96e67f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723e094b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99197091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f84d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d615ee56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerNetModel(\n",
       "  (word_embedding): Embedding(27829, 64)\n",
       "  (lm_head): Linear(in_features=64, out_features=27829, bias=True)\n",
       "  (time_embed): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=512, out_features=768, bias=True)\n",
       "  )\n",
       "  (input_up_proj): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=768, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (input_transformers): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (output_down_proj): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=768, out_features=64, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval().requires_grad_(False).to(dist_util.dev())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "67362550",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_emb = torch.nn.Embedding(\n",
    "        num_embeddings=tokenizer.vocab_size, \n",
    "        embedding_dim=hidden_dim, \n",
    "        _weight=model.word_embedding.weight.clone().cpu()\n",
    "    ).eval().requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9807fbde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## \n",
      "Loading text data...\n",
      "############################## \n",
      "Loading dataset from data...\n",
      "### Loading form the TEST set...\n",
      "### Data samples...\n",
      " ['so shaken as we are so wan with care', 'find we a time for fright peace to pant'] ['find we a time for fright peace to pant', 'and breathe short-wind accents of new broils']\n",
      "RAM used: 4401.46 MB\n",
      "This is raw_datasets:  Dataset({\n",
      "    features: ['src', 'trg'],\n",
      "    num_rows: 60\n",
      "})\n",
      "RAM used: 4401.46 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576fd67e1d294261bac91e694c137202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=4):   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### tokenized_datasets Dataset({\n",
      "    features: ['input_id_x', 'input_id_y'],\n",
      "    num_rows: 60\n",
      "})\n",
      "### tokenized_datasets...example [2, 150, 12941, 142, 137, 205, 150, 7309, 130, 1015, 3]\n",
      "RAM used: 4402.66 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f861a26d094db299a39578e935c008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merge and mask:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 4402.77 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e5e6cc505e44559845dd7ba2484825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "padding:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "    num_rows: 60\n",
      "}) padded dataset\n",
      "RAM used: 4402.96 MB\n",
      "RAM used: 4403.09 MB\n"
     ]
    }
   ],
   "source": [
    "data_valid = load_data_text(\n",
    "        batch_size=20,\n",
    "        seq_len=seq_len,\n",
    "        deterministic=True,\n",
    "        data_dir=data_dir,\n",
    "        split=\"test\",\n",
    "        loaded_vocab=tokenizer,\n",
    "        model_emb=model_emb.cpu(),  # using the same embedding wight with tranining data\n",
    "        loop=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c0aa23ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### End of reading iteration...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(27829, 64)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_test_data = []\n",
    "\n",
    "idx = 0\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        batch, cond = next(data_valid)\n",
    "        # print(batch.shape)\n",
    "        all_test_data.append(cond)\n",
    "        idx += 1\n",
    "\n",
    "except StopIteration:\n",
    "    print('### End of reading iteration...')\n",
    "\n",
    "model_emb.to(dist_util.dev())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fdbe4ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "237c3441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SpacedDiffusion at 0x7fefe75406a0>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0d45d61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_efficient_knn(model_emb, text_emb):\n",
    "    emb_norm = (model_emb**2).sum(-1).view(-1, 1) # vocab\n",
    "    text_emb_t = torch.transpose(text_emb.view(-1, text_emb.size(-1)), 0, 1) # d, bsz*seqlen\n",
    "    arr_norm = (text_emb ** 2).sum(-1).view(-1, 1) # bsz*seqlen, 1\n",
    "    # print(emb_norm.shape, arr_norm.shape)\n",
    "    dist = emb_norm + arr_norm.transpose(0, 1) - 2.0 * torch.mm(model_emb, text_emb_t) # (vocab, d) x (d, bsz*seqlen)\n",
    "    dist = torch.clamp(dist, 0.0, np.inf)\n",
    "    # print(dist.shape)\n",
    "    topk_out = torch.topk(-dist, k=1, dim=0)\n",
    "    return topk_out.values, topk_out.indices\n",
    "\n",
    "def denoised_fn_round(model, text_emb, t):\n",
    "    # print(text_emb.shape) # bsz, seqlen, dim\n",
    "    model_emb = model.weight  # input_embs\n",
    "    # print(t)\n",
    "    old_shape = text_emb.shape\n",
    "    old_device = text_emb.device\n",
    "\n",
    "    if len(text_emb.shape) > 2:\n",
    "        text_emb = text_emb.reshape(-1, text_emb.size(-1))\n",
    "    else:\n",
    "        text_emb = text_emb\n",
    "    # val, indices = get_knn(model_emb, text_emb.to(model_emb.device), dist=dist)\n",
    "    val, indices = get_efficient_knn(model_emb, text_emb.to(model_emb.device))\n",
    "    rounded_tokens = indices[0]\n",
    "    # print(rounded_tokens.shape)\n",
    "    new_embeds = model(rounded_tokens).view(old_shape).to(old_device)\n",
    "\n",
    "    return new_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3c4fbb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1000\n",
    "clip_denoised = False\n",
    "model_kwargs = {}\n",
    "top_p = 0\n",
    "clamp_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "842f74df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82e16a73ee5499c93cdbe4e8afdedaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iterator = iter(all_test_data)\n",
    "word_lst_recover = []\n",
    "word_lst_ref = []\n",
    "word_lst_source = []\n",
    "\n",
    "for cond in iterator:\n",
    "\n",
    "    input_ids_x = cond.pop('input_ids').to(dist_util.dev())\n",
    "    x_start = model.get_embeds(input_ids_x)\n",
    "    input_ids_mask = cond.pop('input_mask')\n",
    "    input_ids_mask_ori = input_ids_mask\n",
    "\n",
    "    noise = torch.randn_like(x_start)\n",
    "    input_ids_mask = torch.broadcast_to(input_ids_mask.unsqueeze(dim=-1), x_start.shape).to(dist_util.dev())\n",
    "    x_noised = torch.where(input_ids_mask == 0, x_start, noise)\n",
    "\n",
    "    model_kwargs = {}\n",
    "\n",
    "    if step == diffusion_steps:\n",
    "        use_ddim = False\n",
    "        step_gap = 1\n",
    "    else:\n",
    "        use_ddim = True\n",
    "        step_gap = diffusion_steps//step\n",
    "\n",
    "    sample_fn = (\n",
    "        diffusion.p_sample_loop if not use_ddim else diffusion.ddim_sample_loop\n",
    "    )\n",
    "\n",
    "    sample_shape = (x_start.shape[0], seq_len, hidden_dim)\n",
    "\n",
    "    samples = sample_fn(\n",
    "        model,\n",
    "        sample_shape,\n",
    "        noise=x_noised,\n",
    "        clip_denoised=clip_denoised,\n",
    "        denoised_fn=partial(denoised_fn_round, model_emb),\n",
    "        model_kwargs=model_kwargs,\n",
    "        top_p=top_p,\n",
    "        clamp_step=clamp_step,\n",
    "        clamp_first=True,\n",
    "        mask=input_ids_mask,\n",
    "        x_start=x_start,\n",
    "        gap=step_gap\n",
    "    )\n",
    "\n",
    "    # print(samples[0].shape) # samples for each step\n",
    "\n",
    "    sample = samples[-1]\n",
    "\n",
    "    # print('decoding for seq2seq', )\n",
    "    # print(sample.shape)\n",
    "\n",
    "    logits = model.get_logits(sample)  # bsz, seqlen, vocab\n",
    "    cands = torch.topk(logits, k=1, dim=-1)\n",
    "\n",
    "#     word_lst_recover = []\n",
    "#     word_lst_ref = []\n",
    "#     word_lst_source = []\n",
    "\n",
    "    # tokenizer = load_tokenizer(args)\n",
    "\n",
    "    for seq, input_mask in zip(cands.indices, input_ids_mask_ori):\n",
    "        len_x = seq_len - sum(input_mask).tolist()\n",
    "        tokens = tokenizer.decode_token(seq[len_x:])\n",
    "        word_lst_recover.append(tokens)\n",
    "\n",
    "    for seq, input_mask in zip(input_ids_x, input_ids_mask_ori):\n",
    "        # tokens = tokenizer.decode_token(seq)\n",
    "        len_x = seq_len - sum(input_mask).tolist()\n",
    "        word_lst_source.append(tokenizer.decode_token(seq[:len_x]))\n",
    "        word_lst_ref.append(tokenizer.decode_token(seq[len_x:]))\n",
    "    break # after 1 batch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8fb4555a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7a7a4756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] so shaken as we are so wan with care [SEP] [SEP]',\n",
       " '[CLS] find we a time for fright peace to pant [SEP] [SEP]',\n",
       " '[CLS] and breathe short - wind accents of new broils [SEP] [SEP]',\n",
       " '[CLS] to be commenc in strands afar remote < eos > [SEP] [SEP]',\n",
       " '[CLS] no more the thirsty entrance of this soil [SEP] [SEP]',\n",
       " \"[CLS] shall daub her lips with her own children's blood [SEP] [SEP]\",\n",
       " '[CLS] nor more shall trench war channel her fields [SEP] [SEP]',\n",
       " '[CLS] nor bruise her flowerets with the arm hoofs [SEP] [SEP]',\n",
       " '[CLS] of hostile paces : those oppos eyes [SEP] [SEP]',\n",
       " '[CLS] which like the meteors of a troubl heaven [SEP] [SEP]',\n",
       " '[CLS] all of one nature of one substance br [SEP] [SEP]',\n",
       " '[CLS] did lately meet in the intestine shock [SEP] [SEP]',\n",
       " '[CLS] and furious close of civil butchery [SEP] [SEP]',\n",
       " '[CLS] shall now in mutual well - beseem ranks [SEP] [SEP]',\n",
       " '[CLS] march all one way and be no more opposed [SEP] [SEP]',\n",
       " '[CLS] against acquaintance kindr and allies : [SEP] [SEP]',\n",
       " '[CLS] the edge of war like an ill - sheath knife [SEP] [SEP]',\n",
       " '[CLS] no more shall cut his master < eos > therefore friends [SEP] [SEP]',\n",
       " '[CLS] as far as to the sepulchre of christ [SEP] [SEP]',\n",
       " '[CLS] whose soldier now under whose bless cross [SEP] [SEP]']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lst_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d9184761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dooms dooms dooms dooms dooms dooms [PAD] dooms [PAD] [PAD] [PAD] [PAD] [PAD] dooms',\n",
       " 'dooms dooms dooms dooms dooms dooms dooms',\n",
       " 'dooms dooms dooms dooms dooms dooms dooms [PAD] dooms dooms [PAD] [PAD] [PAD] dooms',\n",
       " 'dooms dooms dooms dooms dooms dooms dooms [PAD] dooms dooms dooms',\n",
       " 'dooms dooms dooms dooms dooms dooms dooms dooms dooms [PAD] dooms',\n",
       " 'dooms dooms dooms dooms dooms [PAD] dooms dooms [PAD] dooms',\n",
       " 'dooms dooms dooms dooms dooms dooms dooms dooms [PAD] [PAD] [PAD] [PAD] dooms',\n",
       " 'dooms dooms dooms dooms dooms dooms dooms dooms dooms dooms dooms dooms',\n",
       " 'dooms dooms dooms dooms dooms dooms dooms dooms dooms dooms [PAD] dooms',\n",
       " 'dooms dooms dooms dooms dooms dooms dooms [PAD] dooms [PAD] [PAD] dooms [PAD] dooms dooms',\n",
       " 'dooms dooms dooms dooms dooms dooms dooms dooms dooms [PAD] dooms',\n",
       " 'dooms dooms dooms dooms dooms dooms dooms dooms dooms [PAD] dooms',\n",
       " 'dooms dooms dooms dooms dooms dooms dooms dooms dooms dooms [PAD] dooms [PAD] dooms',\n",
       " 'dooms dooms dooms dooms dooms dooms dooms dooms dooms dooms [PAD] [PAD] [PAD] dooms',\n",
       " 'dooms dooms dooms dooms dooms dooms dooms dooms [PAD] [PAD] [PAD] dooms [PAD] dooms',\n",
       " 'dooms dooms dooms dooms dooms dooms [PAD] dooms dooms dooms dooms dooms [PAD] dooms [PAD] dooms',\n",
       " 'dooms dooms dooms dooms dooms dooms dooms [PAD] [PAD] [PAD] dooms',\n",
       " 'dooms dooms dooms dooms dooms dooms dooms',\n",
       " 'dooms dooms dooms dooms dooms dooms dooms dooms dooms dooms [PAD] [PAD] dooms',\n",
       " 'dooms dooms dooms dooms dooms dooms dooms dooms dooms dooms [PAD] dooms [PAD] [PAD] dooms']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lst_recover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "170fa620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] find we a time for fright peace to pant [SEP]',\n",
       " '[CLS] and breathe short - wind accents of new broils [SEP]',\n",
       " '[CLS] to be commenc in strands afar remote < eos > [SEP]',\n",
       " '[CLS] no more the thirsty entrance of this soil [SEP]',\n",
       " \"[CLS] shall daub her lips with her own children's blood [SEP]\",\n",
       " '[CLS] nor more shall trench war channel her fields [SEP]',\n",
       " '[CLS] nor bruise her flowerets with the arm hoofs [SEP]',\n",
       " '[CLS] of hostile paces : those oppos eyes [SEP]',\n",
       " '[CLS] which like the meteors of a troubl heaven [SEP]',\n",
       " '[CLS] all of one nature of one substance br [SEP]',\n",
       " '[CLS] did lately meet in the intestine shock [SEP]',\n",
       " '[CLS] and furious close of civil butchery [SEP]',\n",
       " '[CLS] shall now in mutual well - beseem ranks [SEP]',\n",
       " '[CLS] march all one way and be no more opposed [SEP]',\n",
       " '[CLS] against acquaintance kindr and allies : [SEP]',\n",
       " '[CLS] the edge of war like an ill - sheath knife [SEP]',\n",
       " '[CLS] no more shall cut his master < eos > therefore friends [SEP]',\n",
       " '[CLS] as far as to the sepulchre of christ [SEP]',\n",
       " '[CLS] whose soldier now under whose bless cross [SEP]',\n",
       " '[CLS] we are impress and engag to fight [SEP]']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lst_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20392d44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
